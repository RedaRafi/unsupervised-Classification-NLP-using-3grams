# -*- coding: utf-8 -*-
"""TF-IDF 3-Gram K_Means .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xz2IhbM8dz6qW0KwZSM_kB07yZqwcwtO
"""

import numpy as np
import pandas as pd
import nltk
nltk.download('stopwords')
nltk.download('punkt')


# nltk.download('all')

file ='/content/drive/MyDrive/Boudi DataSet/dataset.csv'
data = pd.read_csv(file,encoding='ISO-8859-1')
data.head()

from nltk.corpus import stopwords
from string import punctuation
import re

def preprocess_intitule(text, debug = False):
  if debug: print(f"Original Text:\n\t {text}")
  tokens = word_tokenize(text.lower(), language='french')
  if debug: print(f"After tokenization:\n\t {tokens}")
  tokens = [c for c in tokens if c not in punctuation]
  if debug: print(f"After removing punctuation:\n\t {tokens}")
  tokens = [w for w in tokens if tokens not in stopwords.words('french') and len(w)>0]
  if debug: print(f"After removing stopwords:\n\t {tokens}")
  if debug: print(f"Final results:\n\t {' '.join(tokens)}")
  return " ".join(tokens)

preprocess_text(data.iloc[0,0], debug=False)

# dropping ALL duplicte values 
data["preprocess_intitule"] = data["intitule"].apply(preprocess_intitule)
# data.drop_duplicates(subset ="preprocess_intitule", keep = "first", inplace = True)

data.size

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(ngram_range=(3,3))
X = tfidf.fit_transform(data['preprocess_intitule'])
X = X * X.T

data



from sklearn.cluster import KMeans
true_k = 11
model = KMeans(n_clusters=true_k, init='k-means++', max_iter=2000)
model.fit(X)
order_centroids = model.cluster_centers_.argsort()[:, ::-1]
# terms = tfidf.get_feature_names()
for i in range(true_k):
  print("********************************************")
  print("Cluster %d:" %i)
  for ind in order_centroids[i, :10]:
    print(data.iloc[ind, 0])

order_centroids.shape
